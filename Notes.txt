#Heart Disease Prediction By Surya Mahesh Kolisetty
#Installing Essential Machine Learning Libraries


!pip install numpy
!pip install pandas
!pip install scikit-learn
!pip install matplotlib
!pip install notebook

#Importing essential libraries


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline

import os
print(os.listdir())

import warnings
warnings.filterwarnings('ignore')

#Importing and Familiarizing with Our Dataset


dataset = pd.read_csv("heart.csv")
##Read and load a CSV dataset named "heart.csv" using the pandas library.

type(dataset)
##Verifying it as a 'dataframe' object in pandas

dataset.shape
##Shape of dataset

dataset.head(5)
##Display first five rows of the dataset

dataset.sample(5)
##Randomly select and display 5 rows of the dataset

dataset.tail(5)
##Display last five rows of the dataset

dataset.describe()
##generates summary statistics of the dataset's numerical columns, such as mean, standard #deviation, min, max, and quartiles, using Pandas.


dataset.info()
##provides a concise summary of the dataset's structure, including the number of non-null entries ##and data types of columns.


dataset.columns
##provides the names of columns in the dataset.

"""
we have seen the column names there,
let's understand columns better
what exactly each column refers and what type of values does each column holds
"""



info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]


for i in range(len(info)):
    print(dataset.columns[i]+":\t\t\t"+info[i])


"""
age:			age
sex:			1: male, 0: female
cp:			chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic
trestbps:			resting blood pressure
chol:			 serum cholestoral in mg/dl
fbs:			fasting blood sugar > 120 mg/dl
restecg:			resting electrocardiographic results (values 0,1,2)
thalach:			 maximum heart rate achieved
exang:			exercise induced angina
oldpeak:			oldpeak = ST depression induced by exercise relative to rest
slope:			the slope of the peak exercise ST segment
ca:			number of major vessels (0-3) colored by flourosopy
thal:			thal: 3 = normal; 6 = fixed defect; 7 = reversable defect

"""

#Analysing target variable


dataset['target']
##target column in dataset


dataset['target'].describe()
##Target variable description

dataset['target'].unique()
##retrieves the unique values present in the "target" column of the dataset.

#Clearly, this is a classification problem, with the target variable having values '0' and '1'


#let's find correlation between columns in the dataset

dataset.corr()["target"].abs().sort_values(ascending=False)
##correlation between the target variable with all other columns of the dataset.

"""
what is meant by correlation???
Correlation refers to a statistical measure that quantifies the relationship between two variables. In the context of data analysis, it indicates how changes in one variable are associated with changes in another variable. Correlation values can range from -1 to 1: 

- A positive correlation (close to 1) suggests that as one variable increases, the other tends to increase as well.
- A negative correlation (close to -1) indicates that as one variable increases, the other tends to decrease.
- A correlation value close to 0 suggests a weak or no linear relationship between the variables.

Correlation is often used to understand the strength and direction of the relationship between variables and is a useful tool in data analysis and feature selection.

"""


print(dataset['target'])
##print target variable in the dataset


target_temp = dataset.target.value_counts()
print(target_temp)
##function returns a Series containing the counts of unique values in descending order.


target_counts = [target_temp[0], target_temp[1]]
target_labels = ['0', '1']

# Creating the bar plot
sns.barplot(x=target_labels, y=target_counts)

# # Adding labels and title
plt.xlabel('Target')
plt.ylabel('Count')
plt.title('Distribution of Target Values')

# # Display the plot
plt.show()

##creates the bar plot based on the target counts and labels. This code will indeed create a bar plot using Seaborn and Matplotlib, visualizing the distribution of the target values in the dataset


total=dataset['target'].count()
patients_without_heart_problems=target_temp[0]
patients_with_heart_problems=target_temp[1]

print("Total Count of patients : ",total)
print("Total Count of patients without heart problems : ",patients_with_no_heart_problems)
print("Total Count of patients with heart problems : ",patients_with_heart_problems)

print("Percentage of patients without heart problems: "+str(round(patients_with_no_heart_problems*100/total,2)))
print("Percentage of patients with heart problems: "+str(round(patients_with_heart_problems*100/total,2)))



### Let's analyse 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca' and 'thal' features


dataset['sex'].unique()
##Analysing the 'sex' feature


###Analysing the 'Chest Pain Type' feature
dataset["cp"].unique()

###Distributions using seaborn and matplotlib
sns.barplot(x=dataset["sex"], y=dataset["target"])
plt.xlabel('sex')
plt.ylabel('target')
plt.title('Distribution of Target Values Based on Sex')
plt.show()

##barplot

sns.distplot(dataset["sex"])
plt.xlabel("sex")
plt.ylabel("target")
plt.title("Distribution of Target values based on sex")
plt.show()

##distplot

##creates a distribution plot using Seaborn, which attempts to show the distribution of values in ##the "sex" column of the dataset.



#Train Test Split using scikit-learn (sklearn)

from sklearn.model_selection import train_test_split

predictors = dataset.drop("target",axis=1)
target = dataset["target"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)


##module to split your dataset into training and testing sets for building and evaluating machine #learning models

##module, which is used for splitting datasets.

##This creates two separate DataFrames/arrays: predictors contains the input features (all columns #except "target"), and target contains the output variable ("target" column).

##This function splits the dataset into training and testing subsets. The parameters are as 
#follows:

##predictors: Input features (X).
##target: Output variable (Y).
##test_size: The proportion of the dataset to include in the test split (here, 20%).
##random_state: A random seed for reproducibility.

"""
These splits are essential for training and evaluating machine learning models to ensure that the model's performance is assessed on unseen data.
"""

#V. Model Fitting
from sklearn.metrics import accuracy_score
#Logistic Regression
from sklearn.linear_model import LogisticRegression
#imporing LogisticRegression from sklearn.linear_model

lr=LogisticRegression()
#create an object fro LogisticRegression

lr.fit(X_train,Y_train)
#fit training data predictors,target variable into LogisticRegression

Y_pred_lr =lr.predict(X_test)
#predict the test data using the trained model


Y_pred_lr.shape

score_lr=round(accuracy_score(Y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using LogisticRegression is: "+str(score_lr)+" %")


"""
Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)

 is a metric commonly used to evaluate the performance of classification models in machine learning. It measures the proportion of correctly predicted instances (or samples) in relation to the total number of instances. In other words, it calculates the ratio of correctly classified samples to the total number of samples in the dataset.

It's important to note that while accuracy can be a useful metric, it might not be sufficient for assessing the performance of a model in cases where classes are imbalanced or when misclassification of specific classes is more critical than others. In such cases, other metrics like precision, recall, F1-score, and confusion matrix should be considered for a more comprehensive evaluation of the model's performance.


"""

#Naive Bayes
from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(X_train,Y_train)

Y_pred_nb = nb.predict(X_test)
Y_pred_nb.shape

score_nb=round(accuracy_score(Y_pred_nb,Y_test)*100,2)

print("The accuracy score achieved using Naive Bayes is: "+str(score_nb)+" %")

#SVM
from sklearn import svm

sv = svm.SVC(kernel='linear')

sv.fit(X_train, Y_train)

Y_pred_svm = sv.predict(X_test)


score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)

print("The accuracy score achieved using Linear SVM is: "+str(score_svm)+" %")

"""
`sv = svm.SVC(kernel='linear')` creates a Support Vector Machine (SVM) classifier instance named `sv` with a linear kernel. SVM is a machine learning algorithm used for classification and regression tasks, and the 'linear' kernel implies that the classifier will use a linear decision boundary to separate different classes in the data.
"""


#K Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,Y_train)
Y_pred_knn=knn.predict(X_test)
Y_pred_knn.shape


score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)

print("The accuracy score achieved using KNN is: "+str(score_knn)+" %")

#Decision Tree
from sklearn.tree import DecisionTreeClassifier

max_accuracy = 0


for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)
    dt.fit(X_train,Y_train)
    Y_pred_dt = dt.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
        
#print(max_accuracy)
#print(best_x)


dt = DecisionTreeClassifier(random_state=best_x)
dt.fit(X_train,Y_train)
Y_pred_dt = dt.predict(X_test)


#Random Forest
from sklearn.ensemble import RandomForestClassifier

max_accuracy = 0


for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x

rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)
from sklearn.ensemble import RandomForestClassifier
​
max_accuracy = 0
​
​
for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
​
rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)
Y_pred_rf.shape
score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
​
print("The accuracy score achieved using Decision Tree is: "+str(score_rf)+" %")
​
#XGBoost
import xgboost as xgb

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, Y_train)

Y_pred_xgb = xgb_model.predict(X_test)
import xgboost as xgb
​
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, Y_train)
​
Y_pred_xgb = xgb_model.predict(X_test)

Y_pred_xgb.shape

score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)

print("The accuracy score achieved using XGBoost is: "+str(score_xgb)+" %")

#Output final score
scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb,score_nn]
algorithms = ["Logistic Regression","Naive Bayes","Support Vector Machine","K-Nearest Neighbors","Decision Tree","Random Forest","XGBoost","Neural Network"]    

for i in range(len(algorithms)):
    print("The accuracy score achieved using "+algorithms[i]+" is: "+str(scores[i])+" %")


sns.set(rc={'figure.figsize':(15,8)})

sns.barplot(x=algorithms,y=scores)
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")
plt.show()


#Finally Random forest hasHighest Accuracy


